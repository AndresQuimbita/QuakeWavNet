{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\programas\\ananconda3\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\paulq\\appdata\\roaming\\python\\python38\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programas\\ananconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\programas\\ananconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programas\\ananconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in d:\\programas\\ananconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in d:\\programas\\ananconda3\\lib\\site-packages (7.6.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in d:\\programas\\ananconda3\\lib\\site-packages (from ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\programas\\ananconda3\\lib\\site-packages (from ipywidgets) (5.0.5)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in d:\\programas\\ananconda3\\lib\\site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in d:\\programas\\ananconda3\\lib\\site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from ipywidgets) (7.22.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from ipywidgets) (1.0.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: jupyter-client in d:\\programas\\ananconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in d:\\programas\\ananconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: decorator in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.0.6)\n",
      "Requirement already satisfied: pickleshare in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.17)\n",
      "Requirement already satisfied: pygments in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: backcall in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in d:\\programas\\ananconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: ipython-genutils in d:\\programas\\ananconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in d:\\programas\\ananconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in d:\\programas\\ananconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in d:\\programas\\ananconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: six>=1.11.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: jinja2 in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
      "Requirement already satisfied: pyzmq>=17 in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (19.0.2)\n",
      "Requirement already satisfied: argon2-cffi in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.0.7)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.9.4)\n",
      "Requirement already satisfied: prometheus-client in d:\\programas\\ananconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\programas\\ananconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets) (227)\n",
      "Requirement already satisfied: wcwidth in d:\\programas\\ananconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=0.5 in d:\\programas\\ananconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programas\\ananconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.3)\n",
      "Requirement already satisfied: bleach in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: testpath in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: defusedxml in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in d:\\programas\\ananconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: pycparser in d:\\programas\\ananconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: async-generator in d:\\programas\\ananconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in d:\\programas\\ananconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: packaging in d:\\programas\\ananconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
      "Requirement already satisfied: webencodings in d:\\programas\\ananconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Collecting pytorch-lightning==1.5.10\n",
      "  Using cached pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.7.* in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (2.1.0)\n",
      "Requirement already satisfied: future>=0.17.1 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (5.4.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (2023.6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (2.12.3)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (1.2.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (0.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (20.9)\n",
      "Requirement already satisfied: typing-extensions in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (4.8.0)\n",
      "Requirement already satisfied: setuptools==59.5.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning==1.5.10) (59.5.0)\n",
      "Requirement already satisfied: requests in d:\\programas\\ananconda3\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2.25.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\programas\\ananconda3\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.8.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from packaging>=17.0->pytorch-lightning==1.5.10) (2.4.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.54.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.4.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (4.22.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.36.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\paulq\\appdata\\roaming\\python\\python38\\site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.6.0)\n",
      "Requirement already satisfied: sympy in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (1.8)\n",
      "Requirement already satisfied: networkx in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (2.5)\n",
      "Requirement already satisfied: jinja2 in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (2.11.3)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in d:\\programas\\ananconda3\\lib\\site-packages (from torchmetrics>=0.4.1->pytorch-lightning==1.5.10) (0.9.0)\n",
      "Requirement already satisfied: colorama in d:\\programas\\ananconda3\\lib\\site-packages (from tqdm>=4.41.0->pytorch-lightning==1.5.10) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (20.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\programas\\ananconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (6.6.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programas\\ananconda3\\lib\\site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2020.12.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programas\\ananconda3\\lib\\site-packages (from jinja2->torch>=1.7.*->pytorch-lightning==1.5.10) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\programas\\ananconda3\\lib\\site-packages (from networkx->torch>=1.7.*->pytorch-lightning==1.5.10) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programas\\ananconda3\\lib\\site-packages (from sympy->torch>=1.7.*->pytorch-lightning==1.5.10) (1.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programas\\ananconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in d:\\programas\\ananconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.2.2)\n",
      "Installing collected packages: pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 1.9.5\n",
      "    Uninstalling pytorch-lightning-1.9.5:\n",
      "      Successfully uninstalled pytorch-lightning-1.9.5\n",
      "Successfully installed pytorch-lightning-1.5.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lightning-bolts 0.7.0 requires pytorch-lightning<2.0.0,>1.7.0, but you have pytorch-lightning 1.5.10 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-ml-py3 in d:\\programas\\ananconda3\\lib\\site-packages (7.352.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neptune-client in d:\\programas\\ananconda3\\lib\\site-packages (1.8.2)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (3.1.40)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (8.2.0)\n",
      "Requirement already satisfied: PyJWT in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (2.8.0)\n",
      "Requirement already satisfied: boto3>=1.16.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (1.28.73)\n",
      "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (11.0.3)\n",
      "Requirement already satisfied: click>=7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (8.1.3)\n",
      "Requirement already satisfied: future>=0.17.1 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (0.18.2)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (3.2.2)\n",
      "Requirement already satisfied: packaging in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (20.9)\n",
      "Requirement already satisfied: pandas in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (1.2.4)\n",
      "Requirement already satisfied: psutil in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (5.8.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (2.25.1)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (1.3.1)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (1.15.0)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (3.0.3)\n",
      "Requirement already satisfied: urllib3 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (1.26.4)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in d:\\programas\\ananconda3\\lib\\site-packages (from neptune-client) (1.6.4)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.73 in d:\\programas\\ananconda3\\lib\\site-packages (from boto3>=1.16.0->neptune-client) (1.31.73)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\programas\\ananconda3\\lib\\site-packages (from boto3>=1.16.0->neptune-client) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from boto3>=1.16.0->neptune-client) (0.7.0)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.1.0)\n",
      "Requirement already satisfied: msgpack in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.2)\n",
      "Requirement already satisfied: python-dateutil in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.1)\n",
      "Requirement already satisfied: pyyaml in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (5.4.1)\n",
      "Requirement already satisfied: simplejson in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.19.2)\n",
      "Requirement already satisfied: monotonic in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
      "Requirement already satisfied: typing-extensions in d:\\programas\\ananconda3\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (4.8.0)\n",
      "Requirement already satisfied: colorama in d:\\programas\\ananconda3\\lib\\site-packages (from click>=7.0->neptune-client) (0.4.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\programas\\ananconda3\\lib\\site-packages (from GitPython>=2.0.8->neptune-client) (4.0.11)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from requests>=2.20.0->neptune-client) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\programas\\ananconda3\\lib\\site-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programas\\ananconda3\\lib\\site-packages (from requests>=2.20.0->neptune-client) (2020.12.5)\n",
      "Requirement already satisfied: jsonschema in d:\\programas\\ananconda3\\lib\\site-packages (from swagger-spec-validator>=2.7.4->neptune-client) (3.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from packaging->neptune-client) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\programas\\ananconda3\\lib\\site-packages (from pandas->neptune-client) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\programas\\ananconda3\\lib\\site-packages (from pandas->neptune-client) (1.23.5)\n",
      "Requirement already satisfied: jsonref in d:\\programas\\ananconda3\\lib\\site-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\programas\\ananconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.17.3)\n",
      "Requirement already satisfied: setuptools in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (59.5.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.4)\n",
      "Requirement already satisfied: rfc3987 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
      "Requirement already satisfied: strict-rfc3339 in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.7)\n",
      "Requirement already satisfied: webcolors in d:\\programas\\ananconda3\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning-bolts in d:\\programas\\ananconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: numpy in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-bolts) (1.23.5)\n",
      "Collecting pytorch-lightning<2.0.0,>1.7.0 (from lightning-bolts)\n",
      "  Using cached pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
      "Requirement already satisfied: torchmetrics in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-bolts) (1.2.0)\n",
      "Requirement already satisfied: lightning-utilities>0.3.1 in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-bolts) (0.9.0)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-bolts) (0.16.0)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-bolts) (2.12.3)\n",
      "Requirement already satisfied: packaging>=17.1 in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-utilities>0.3.1->lightning-bolts) (20.9)\n",
      "Requirement already satisfied: typing-extensions in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-utilities>0.3.1->lightning-bolts) (4.8.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.1.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (5.4.1)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in d:\\programas\\ananconda3\\lib\\site-packages (from pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2023.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.54.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.4.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (4.22.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (59.5.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel>=0.26 in d:\\programas\\ananconda3\\lib\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (0.36.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\programas\\ananconda3\\lib\\site-packages (from torchvision>=0.10.0->lightning-bolts) (8.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\paulq\\appdata\\roaming\\python\\python38\\site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.6.0)\n",
      "Requirement already satisfied: sympy in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.8)\n",
      "Requirement already satisfied: networkx in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.5)\n",
      "Requirement already satisfied: jinja2 in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (2.11.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\programas\\ananconda3\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.8.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (1.15.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programas\\ananconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\programas\\ananconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->lightning-bolts) (6.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from packaging>=17.1->lightning-utilities>0.3.1->lightning-bolts) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\programas\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programas\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programas\\ananconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.9.1->lightning-bolts) (2020.12.5)\n",
      "Requirement already satisfied: colorama in d:\\programas\\ananconda3\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (20.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programas\\ananconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programas\\ananconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->lightning-bolts) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in d:\\programas\\ananconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->lightning-bolts) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programas\\ananconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->lightning-bolts) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programas\\ananconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\programas\\ananconda3\\lib\\site-packages (from networkx->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programas\\ananconda3\\lib\\site-packages (from sympy->torch>=1.10.0->pytorch-lightning<2.0.0,>1.7.0->lightning-bolts) (1.2.1)\n",
      "Installing collected packages: pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 1.5.10\n",
      "    Uninstalling pytorch-lightning-1.5.10:\n",
      "      Successfully uninstalled pytorch-lightning-1.5.10\n",
      "Successfully installed pytorch-lightning-1.9.5\n",
      "Requirement already satisfied: torchmetrics in d:\\programas\\ananconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in d:\\programas\\ananconda3\\lib\\site-packages (from torchmetrics) (1.23.5)\n",
      "Requirement already satisfied: torch>=1.8.1 in d:\\programas\\ananconda3\\lib\\site-packages (from torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in d:\\programas\\ananconda3\\lib\\site-packages (from torchmetrics) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\programas\\ananconda3\\lib\\site-packages (from torchmetrics) (4.8.0)\n",
      "Requirement already satisfied: packaging>=17.1 in d:\\programas\\ananconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\paulq\\appdata\\roaming\\python\\python38\\site-packages (from torch>=1.8.1->torchmetrics) (3.6.0)\n",
      "Requirement already satisfied: sympy in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (1.8)\n",
      "Requirement already satisfied: networkx in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.5)\n",
      "Requirement already satisfied: jinja2 in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2.11.3)\n",
      "Requirement already satisfied: fsspec in d:\\programas\\ananconda3\\lib\\site-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\programas\\ananconda3\\lib\\site-packages (from packaging>=17.1->lightning-utilities>=0.8.0->torchmetrics) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programas\\ananconda3\\lib\\site-packages (from jinja2->torch>=1.8.1->torchmetrics) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\programas\\ananconda3\\lib\\site-packages (from networkx->torch>=1.8.1->torchmetrics) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programas\\ananconda3\\lib\\site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for ant-colony: 'utf-8' codec can't decode byte 0x9c in position 2: invalid start byte in METADATA file at path: d:\\programas\\ananconda3\\lib\\site-packages\\ant_colony-0.1.2.dist-info\\METADATA\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ipywidgets\n",
    "!pip install pytorch-lightning==1.5.10\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install neptune-client\n",
    "!pip install lightning-bolts\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\pkg_resources\\__init__.py:116: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\pkg_resources\\__init__.py:116: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  warnings.warn(\n",
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2Config\n",
    "import torch\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import Wav2Vec2FeatureEncoder, Wav2Vec2NoLayerNormConvLayer, Wav2Vec2LayerNormConvLayer\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "import ipywidgets\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torchaudio\n",
    "import torchtext\n",
    "import pytorch_lightning as pl\n",
    "import nvidia_smi\n",
    "from pytorch_lightning.loggers.neptune import NeptuneLogger\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from IPython.display import display, HTML\n",
    "from dataclasses import dataclass, field\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import F1Score\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if the GPU is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 29 11:42:02 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.79                 Driver Version: 531.79       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   58C    P8                4W /  N/A|    747MiB /  4096MiB |      8%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1908    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A      5824    C+G   ...on\\118.0.2088.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      6372    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      7460    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A      7604    C+G   ...on\\118.0.2088.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     11424    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     11560    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11684    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12364    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15636    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     15656    C+G   ...Brave-Browser\\Application\\brave.exe    N/A      |\n",
      "|    0   N/A  N/A     15704    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16620    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     17588    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     18948    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     20404    C+G   ...82.0_x64__zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     21340    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     23524    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Lightning Version: 1.9.5\n",
      "Device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pytorch Lightning Version: {pl.__version__}\")\n",
    "nvidia_smi.nvmlInit()\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "print(f\"Device name: {nvidia_smi.nvmlDeviceGetName(handle)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': 'wav2vec2-sound_sismic_train',\n",
       " 'lr': 1e-05,\n",
       " 'w_decay': 0,\n",
       " 'bs': 16,\n",
       " 'patience': 30,\n",
       " 'hold_epochs': 20,\n",
       " 'accum_grads': 4,\n",
       " 'pretrained': 'facebook/wav2vec2-base-960h',\n",
       " 'wav2vec2_processor': 'facebook/wav2vec2-base-960h',\n",
       " 'freeze_finetune_updates': 0,\n",
       " 'warmup_epochs': 40,\n",
       " 'apply_mask': True,\n",
       " 'mask_time_length': 10,\n",
       " 'max_epochs': 300}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = \"wav2vec2-sound_sismic_train\" #@param {type: \"string\"}\n",
    "lr = 1e-5#@param {type: \"number\"}\n",
    "w_decay = 0#@param {type: \"number\"}\n",
    "bs = 16#@param {type: \"integer\"}\n",
    "accum_grads = 4#@param {type: \"integer\"}\n",
    "patience = 30#@param {type: \"integer\"}\n",
    "max_epochs = 300#@param {type: \"integer\"}\n",
    "warmup_steps = 1000#@param {type: \"integer\"}\n",
    "hold_epochs = 20#@param {type: \"integer\"}\n",
    "pretrained = \"facebook/wav2vec2-base-960h\"#@param {type: \"string\"}\n",
    "wav2vec2_processor = \"facebook/wav2vec2-base-960h\"#@param {type: \"string\"}\n",
    "freeze_finetune_updates = 0#@param {type: \"integer\"}\n",
    "warmup_epochs = 40#@param {type: \"integer\"}\n",
    "apply_mask=True#@param {type: \"boolean\"}\n",
    "mask_time_length= 10#@param {type: \"integer\"}, era 1\n",
    "\n",
    "# Define hyperparameters\n",
    "hparams = {\"version\": version,\n",
    "          \"lr\": lr,\n",
    "          \"w_decay\": w_decay,\n",
    "          \"bs\": bs,\n",
    "          \"patience\": patience,\n",
    "          \"hold_epochs\":hold_epochs,\n",
    "          \"accum_grads\": accum_grads,\n",
    "          \"pretrained\":pretrained,\n",
    "          \"wav2vec2_processor\": wav2vec2_processor,\n",
    "          \"freeze_finetune_updates\":freeze_finetune_updates,\n",
    "          \"warmup_epochs\":warmup_epochs,\n",
    "          \"apply_mask\":apply_mask,\n",
    "          \"mask_time_length\":mask_time_length,\n",
    "          \"max_epochs\": max_epochs}\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the original processor from Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b59b14d7924cc0893101c411cb64c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\paulq\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241b892182e64db4b8371a19e9a52b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3187349e724997b151ff0dfa39fb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418ade2bc5914c0f980b0d53f4374ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f822a89cde403a9adadfd6b475594c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(hparams[\"wav2vec2_processor\"], return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2Processor:\n",
      "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='facebook/wav2vec2-base-960h', vocab_size=32, model_max_length=9223372036854775807, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the model to accept n channels instead of just 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2GroupNormConvLayer(nn.Module):\n",
    "    def __init__(self, config, num_input_channels=1, layer_id=0):\n",
    "        super().__init__()\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else self.num_input_channels\n",
    "        self.out_conv_dim = config.conv_dim[layer_id]\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            self.in_conv_dim,\n",
    "            self.out_conv_dim,\n",
    "            kernel_size=config.conv_kernel[layer_id],\n",
    "            stride=config.conv_stride[layer_id],\n",
    "            bias=config.conv_bias,\n",
    "        )\n",
    "        self.activation = ACT2FN[config.feat_extract_activation]\n",
    "\n",
    "        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.conv(hidden_states)\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "        hidden_states = self.activation(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class Wav2Vec2_ChannelFeatureEncoder(nn.Module):\n",
    "    \"\"\"Construct the features from raw audio waveform\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_input_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_input_channels = num_input_channels\n",
    "        \n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers = [Wav2Vec2GroupNormConvLayer(config, num_input_channels= self.num_input_channels,layer_id=0)] + [\n",
    "                Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Wav2Vec2LayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.gradient_checkpointing = False\n",
    "        self._requires_grad = True\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._requires_grad = False\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        hidden_states = input_values[:] # mudou para que receba todos os canais (4)\n",
    "        #print(\"hidden_states\", hidden_states.shape)\n",
    "\n",
    "        # make sure hidden_states require grad for gradient_checkpointing\n",
    "        if self._requires_grad and self.training:\n",
    "            hidden_states.requires_grad = True\n",
    "\n",
    "        for conv_layer in self.conv_layers:\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(conv_layer),\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "# Crio o novo modelo que herda os processos de Wav2Vec2, mas usa o extrator de features baseado em 4 canais\n",
    "class Wav2Vec2_ChannelModel(Wav2Vec2Model):\n",
    "    def __init__(self, config: Wav2Vec2Config, num_input_channels=1):\n",
    "        super().__init__(config)\n",
    "\n",
    "        #del self.feature_extractor\n",
    "        self.feature_extractor = Wav2Vec2_ChannelFeatureEncoder(config, num_input_channels=num_input_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab875e336365475d9bde9c4adbcaabfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2_ChannelModel: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'lm_head.bias', 'lm_head.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2_ChannelModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2_ChannelModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2_ChannelModel were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2_ChannelModel.from_pretrained(\"facebook/wav2vec2-base-960h\",\n",
    "                                                 conv_dim = (512, 512, 512,512,512,512),\n",
    "                                                 conv_stride = (5, 2, 2,2,2,2),\n",
    "                                                 conv_kernel = (10, 3, 3,3,3,2),\n",
    "                                                 num_feat_extract_layers = 6,\n",
    "                                                 num_input_channels = 1,\n",
    "                                                 ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2_ChannelModel(\n",
      "  (feature_extractor): Wav2Vec2_ChannelFeatureEncoder(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Wav2Vec2GroupNormConvLayer(\n",
      "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (5): Wav2Vec2NoLayerNormConvLayer(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (feature_projection): Wav2Vec2FeatureProjection(\n",
      "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Wav2Vec2Encoder(\n",
      "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "      (conv): ParametrizedConv1d(\n",
      "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "        (parametrizations): ModuleDict(\n",
      "          (weight): ParametrizationList(\n",
      "            (0): _WeightNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (padding): Wav2Vec2SamePadLayer()\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
      "        (attention): Wav2Vec2Attention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Wav2Vec2FeedForward(\n",
      "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO THE TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-79cb622c246c>:31: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file, sr=None)\n",
      "d:\\Programas\\Ananconda3\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E:/TESIS/DataSet/AudioSismig/63e65c8adae78edb6ca641a368ae9cc1_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/424df27a90cd2e79da4c8db8b0c5cf12_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/838b0472787cfcdbfbbf3b8f3e035e2d_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f01547b1946b1922e4f6876fd40ce9da_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/eadce4cf2ff973534d62f3b4940eb4a4_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c8e1c4f29035a394492ea4d603eafea3_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/406ab147a90b09e29f51852584f47db6_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/508064df8e817ad664a012e558a2782a_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a74fdea1bbb9e5ef0091c803de836c95_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d415433dd8f7fea18a2f5ecf69a63a2e_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/beb17c037f2ea59f6d849756a8b69344_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f91ec4d82f9d8ba8b7ead9a046c4a1c9_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b95f7f47654f06a812e14b21fca224b5_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/5f290b0222c4a4d87fa4dcf8fa64fbd1_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/1b1c859bb0b490ec11836ed78256f14e_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d38b2ef9405e65a590d7c653eea547e1_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/e0309e5ec387f12c562f09afe2dfc578_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/45a02107582cb0e53216e76f29680f4f_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b9470de60cfedef4200190401ff70fa4_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b4b626e1f494b225b704ab927914ef17_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/659d423a1e39f950b1dffbcd231844ee_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/13a5d2540c6cf653cdfc41f83dfe1252_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4014630c04f9ac20d2ed21746897f51b_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/9e688f8f48e325ffef806fd7b007dbef_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f1e77ae638c6ff67e3f018e59bb4b899_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/20ae2018eb7ee706403ef050c1b8b3ea_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f325ed5978be1998ea9d38c9a4e685bc_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/9b67ce145ead10f581c93b5c4e930e48_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/00ccc61c33c1604e67cea67133b055d5_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/acf791ed878c77621a004f318970f8f0_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/44a06450d9ff8031e11743c6c0078ac3_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a3263354bb81a1b66779c723796ea3db_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ece97350bfb7c4e84dbfbf40d8cb1802_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fb99d97d0ed1e6f7f720c0094dbc6e5c_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f8d2393617e9999805ebb776a9a9e1c7_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c117712b27db6105b2f8f4a26027f96a_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/5309eeb2659aa2d416caab6f818f9727_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4287894caeaf67e77f95098b189af3b9_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/e0b692ea61d3b3e6e58ff83f3ad64fbc_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cd370b982adf6d46942c32eafb802b38_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/86eb0075f93e22f63d692e0cc33b82c6_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cfc652a40f0de0e38282f324592247fa_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f6736f2d409fb777332e76f4511b0819_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fdd1b5229e5fdbd970592fd0456f58aa_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/bbcf4cc98eab5464924a5b20867a3000_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fcb1e1503bd3faa9e7c23b256b848ed5_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/3b2015c004b3dfdbccc83d431c8a4d0b_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b217396e32a35a53d9ad19bae60a41ad_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ad98b9d6ebd6b49498b16e5e2628ca60_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cab1a91f58899d7a3ae3955dc1e3edea_BHZ_BREF_HB.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2e52b752f496ad2cb7c652abf042d240_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cab1a91f58899d7a3ae3955dc1e3edea_BHN_BREF_HB.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/39a832d24095ad996149c205de876cbf_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/aa4eb30b8abce8d4584c1b727798c83a_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/7bfd6d4e1e100c6bafafa4c76947c3dd_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d5f81d2360755024b5ac1d1323958baa_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/60ee65ffac943b0a23e8ff3d51dc6139_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/621627107dc49f6cbfc570a39dadd571_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/3e43d5d70b7a4ffc8e5b6c2d9c92f65d_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4cfe37d3c55e298d31ccca5b0fe902e1_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c230cddb73efbf54a31e06f3d051b2a5_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/9f6dc7270a1a0fa2cb9b456245bfa300_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/91c6f347697ac96ce0ed1fa923f01cd7_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/8657a2f9e79133b0299e071e305a487f_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2368af243c1174878ad285e3f6766cbd_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c6bcf2fc3999eb3e03c8e44116b933ac_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2055270109d98307a98342ecddb8982e_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d1fe515eb65606805c4e4ba7bf53f5f9_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/069e91ee0d86b266de7b65a08e48ef25_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/58dec915933daa833cb834ada00489a2_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/bb151e22116ace9787e8eff1b9de100e_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0f09e1df1694b05c08777dd0e6d8657f_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c40635ef5cade7fb6bc05947a8b1b5ab_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b0530434bb8c7116b62fac87ec8f4009_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4bab9caf2721c43d4bbb56d601f6f572_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/22409a6167c07f9dee00d5fd54210ac9_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6553cc4b36b149a106298ec4f6f51850_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/64c1a30c271e8daa2449695f11271458_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/3fd8ab320c5c48a5cede2cd90cd7f751_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/3a6e9deabf486655c3dcae8282af60f4_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4c5b92a6cd07d0488733dd877324e82c_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/25a5c6ca0315558bf88ba8bda4f26755_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6122f7dd01283c8e15d1cb81ee2c7ec8_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a8e1f0eba96470fd4701f0348a478205_BHZ_BREF_HB.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b64e75f56465725687f0a3012b14f366_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/aa6108d75f754fa86dcf9c8ae30d65e3_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ddf483c0d7a8fe94f0470cea98fcceca_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/99caca85c52926a9bf18e82f5c9e31ef_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/67d6bf7b4160f79d7afe73d9c9659260_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/183ea646764f9e589f78387482406ff9_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c1905c08e99cf3762adb636496bccc48_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/9d76b502feb9a8b7d8cb49f3cfabfc66_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/920d32179c8fc49eaffc5611022999a1_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/767f4d630283ebd4a850f6e47bd2163a_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ece97350bfb7c4e84dbfbf40d8cb1802_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/e125479d3e414859c9f7a77bbf318db4_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/5976b94982fb83417df7e1be5d40e1a7_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c230cddb73efbf54a31e06f3d051b2a5_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fbc815464e0c545de80a4af10387e138_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/881f5826bc1949be517240fbe02a60bb_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f0751ccaa242d2eacb3b9be7d0d95c25_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f8c7e29a442d9100a0c5c17709e5155e_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/21dec5b84edcd34ebf754435d6aebee9_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6b96450ae0ab730816620fa2746fa582_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/9b9ea89fc2296664fea880be78bfcf7b_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/366bac8e2557143a7278cddf05107487_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a2953433662033c22ee698043d7a2ca0_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/731258295151393e07bc69490d3f446d_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/873a79c2a47c59e5081e15c0a4cb16d7_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0586689e81699b085604c765f476c553_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/be5f21802d676edf3c8ac7b6949868eb_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b6d36eeb9dc0d3f0794102b0e4159f20_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a2953433662033c22ee698043d7a2ca0_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4d7791b2ffbffedab4f092225764c2a0_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/28d17adc851e8d5b139171fc56bd406c_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f14a125f7246e50d3dac7163bda3863b_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/257cda2de37e11a872aa9b9f901414cd_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fcd8e692415ef9195586d7c186048de1_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/7fb934457b8552ab38a71a1d39574bf2_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6c878d869bcee8de0b8abe108b5828ae_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/44b3edbf93bf2d75f1a2bd92bcf7f825_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fcd8e692415ef9195586d7c186048de1_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/00a4500f57193b2889b5c4967213eb5b_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/54f2d1f50aacbde9154ff37968bc0463_BHZ_BREF_HB.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/10924093b1c2682f39ff4e3623926617_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/3b6639e0582b56ffc52d5ea1179ccf5e_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/e3d538fc5e5494edf6baf5333766c38b_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ff6667436f3a3a206d1f118076c80e34_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/95aac4adfdf7ff3fa7a70774e50a7767_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/36384a7bd411c45a7c3651277b01ed92_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d2179bb522a56d4c05b371155c644418_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d2550a5f11c7bc1adbf6bceb3dd69d56_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/618017889e3d9453fd405fdcce28060d_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ebf35c5679eb2c3b60aba0d7ad564ed1_BHN_BREF_HB.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fecd2eadbd83df4c401f5143bb8b2496_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/116dc0833a0a5d6ec73634e14d23d8d3_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/e49e05e2e483517f31031a8baa407ece_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4c5b92a6cd07d0488733dd877324e82c_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/08ec368141c0fa881ae27d64bbaaace7_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d319d6929a63271d7281a204aba509d6_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2ac0448699372ce9c9b983f81d75c693_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/1afaa0ddc8fb9ba09b8955eb3a4c368e_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ffb339004856c01825d3a8124df020d4_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/96824463680d1e8e07c0b903a1d3aeda_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c4cd8363f07a5f5a62b05e87e1d03045_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fba9e6a52337aa0de5023a4264894b33_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/acaabf4b8d88dea16c02018f2242d738_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6b96450ae0ab730816620fa2746fa582_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cb84470226bb4dff1c49d1ae6ae0ade5_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0dd6b239defd5b3bce7b5f0d6d55129e_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/eb14905c72fae91eac9e7c32968ab8cb_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c599624cf0cafda13f5f7cc3ff781c8b_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/03ebcaf6285661bf70a537b7b26c6b8b_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/eebfe1f9ab940eb0f1489ac7cd45e930_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/220a4d0114096a34504610d4f8604cd2_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/8a64d52bed2004db203695e62063307c_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/63691082f42b11c58f0e6a06d588d58f_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a211d1b11cd09962548181ff6581500b_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/873937f52188c87a5901ea870f18a137_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/fba378180dbbf0b983dbf70dccb5d6f9_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d640304697ecf2940326bd09950a5357_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4058e2d04d222b31b02f0db2d78565c8_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/bfeb50b5eeb516a8db5bc2bcb820816a_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0e34a2b7402472b871b56406b78a2db1_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d38b2ef9405e65a590d7c653eea547e1_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cec80ad1fb8c3b089edc5626ff02a9fb_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cc188e915004a831898eb56916c88d16_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/bb4c3b7105f63159eeca8d4831e56f45_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cb158d9ac37376010dfa82d38b639ef9_BDF_BNAS_VLP_TYPE1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-79cb622c246c>:38: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file, sr=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c0463d86042564551956314e85c70ec6_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/794ef03d6a56e6fe07e1528e2387bd06_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0ab4a8099b356b53644cdbbe2957121e_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/346483c520ba9e3c264b3686adef21db_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0002a32d9f7576599e5f41f7333c0098_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b95f7f47654f06a812e14b21fca224b5_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2e583f7bb7e30c9385f8793685f25cb6_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/51faf8e5cab4e392a9728be06576d63b_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/920d32179c8fc49eaffc5611022999a1_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/14f881550a7a283acf1118cc2844f55f_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/5f86c4bbfd4e783ff0a3a1d51e8be597_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4aecb0ba2b133595fd4ed2b3994907fa_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/65b96e897a4e6dc11c0590eb7783cfd2_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/3b782c9871912e21c25f825d9e6d6502_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/96415914ec4ff3adcfb2c97820b54fc9_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d3fb1e96adf18f6f64c32c6ea770f55e_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/8aa5a3d31c3ca6f8d651240015b0c553_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/556e187fe28573ef397d743d934355fb_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/1dfd34405b95d37ab9cfa13f6063b941_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/17e376870b53369bfe5d29240fc30cb8_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/8d029273857a00ca9dea5c9785ee8cbb_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ab40594f9e383568845189824ec3f49f_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/85e1c0f55f4e3f35b672274c94276af9_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/14c02ce3a54c310633bbff8240a0bcc0_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/d0263610a24869fbf81763617c94387c_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b6d36eeb9dc0d3f0794102b0e4159f20_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4856b0ba0e4f4c8c40af8a1eda969072_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f38f26ebf6f7127af1f21ab4895f2d95_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/0022fe2159aea12d4e486db9d21ebdef_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b04902b4a64a35ee7ff16a8eb5c64834_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/50e20817af8b4ab4d1fb3282c57d0a81_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a4414d4ac3ab8850813abcd9263ace2e_BDF_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2c45de6e3eb2de1d377424952ef53114_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c002c871afc37a463f445fb21aeb0c90_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/af3b85d7bfbde370df02488d3d636a29_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/053ff8ca4de8576122fb1177faa1c90f_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/c957811fd073f6d319100df737d9aa14_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/87c47fca24d1ab694f51a83a8c2a11e8_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f99a59318041417304cf2d5fa7689830_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a85dcee30d279cd18e5fef4cab8a9071_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4cceaae5d3422449ce76d7416128cf43_BHN_BTAM_LP.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-79cb622c246c>:46: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file, sr=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4d7148e0aae975fe48c0ee610ac285ca_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/1fecb2c616ee2869a02c853fde528083_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/bd73b3338746b508015c39df899f7b4d_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/7fcd1b4245118c4c7e2bff681e4f267b_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b24b7e52f50cd3ecb58828853f15340f_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f8feb376ecaf69684add132a25b01073_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ed31672125758dac5313e525af133825_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/865065924c9caf8849cd3b9372588dc9_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4aecb0ba2b133595fd4ed2b3994907fa_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/4de1ce75862558c9a2debd0e7c08e755_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/ff6667436f3a3a206d1f118076c80e34_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/f0ea2d4f1b76874a87fde499f3837d75_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6fa31462ef2d772e67cac026f4a9fec8_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/093ce27d7eaa1d4704cddba62dce1ffd_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/546128b2cb6fb5f6f30ead7e0d751651_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/cf7bf0233da3aa673d86975b1fecff18_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/26ffecc52fa289152de022736237e996_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b824a35a1370eb5a08956b31576f2e5b_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/7408e79f8455541ad14b7fec77f75420_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/b2d08d85e927481dd8fa77189c4917ed_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/a98df5a948aa8bbac5387ce71e520552_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/aefc0873a71fdd8342729da8a4d27151_BHE_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/6d1312796c4a441feadc4700660b07dc_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/7fdf7cb8b53efabb5d097d1a676484f2_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/90fce428bcd00a7de3f13dc32f09bcf7_BHN_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/463250881873feecc006ccbaa62c77b0_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/46778ace1e2185931efa4a028706a595_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/aeabc0dd6fabbf2d4f8e5226a7f96f64_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/7a63de82aba7ab760ba2090a8d79250d_BDF_BNAS_VLP_TYPE1.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/2efa11f61cf6edb93affe2ae5355b503_BHE_BNAS_VLP_TYPE2.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/43779f7d391251e2d9d9784bb4d0d2d1_BHZ_BTAM_LP.wav\n",
      "\n",
      "E:/TESIS/DataSet/AudioSismig/79b58bab12e470870d3ca7f94c904fb2_BHE_BNAS_VLP_TYPE1.wav\n",
      "Data split completed.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import os\n",
    "# import shutil\n",
    "# import librosa\n",
    "\n",
    "# #Define la ruta de la carpeta que contiene los archivos de audio\n",
    "# input_folder = 'E:/TESIS/DataSet/AudioSismig/'\n",
    "\n",
    "# #Lista todos los archivos de la carpeta\n",
    "# files = [os.path.join(input_folder, file) for file in os.listdir(input_folder)]\n",
    "\n",
    "# #Define las proporciones para train, test y validation sets\n",
    "# train_ratio = 0.7\n",
    "# test_ratio = 0.15\n",
    "# validation_ratio = 0.15\n",
    "\n",
    "# ##Divide los datos en train, test y validation sets\n",
    "# train_files, temp_files = train_test_split(files, test_size=1 - train_ratio)\n",
    "# test_files, validation_files = train_test_split(temp_files, test_size=validation_ratio / (test_ratio + validation_ratio))\n",
    "\n",
    "# #Define las carpetas de salida\n",
    "# output_folder = './data/'\n",
    "# os.makedirs(os.path.join(output_folder, 'train'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(output_folder, 'test'), exist_ok=True)\n",
    "# os.makedirs(os.path.join(output_folder, 'validation'), exist_ok=True)\n",
    "\n",
    "\n",
    "# #Copia los archivos a las carpetas correspondientes\n",
    "# for file in train_files:\n",
    "#     try:\n",
    "#         audio, _ = librosa.load(file, sr=None)\n",
    "#         shutil.copy(file, os.path.join(output_folder, 'train'))\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(file)\n",
    "# for file in test_files:\n",
    "#     try:\n",
    "#         audio, _ = librosa.load(file, sr=None)\n",
    "#         shutil.copy(file, os.path.join(output_folder, 'test'))\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(file)\n",
    "    \n",
    "# for file in validation_files:\n",
    "#     try:\n",
    "#         audio, _ = librosa.load(file, sr=None)\n",
    "#         shutil.copy(file, os.path.join(output_folder, 'validation'))\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(file)\n",
    "\n",
    "# print(\"Data split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANSYN_Dataset_SE(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, filenames, audio_path):\n",
    "        self.filenames = filenames\n",
    "        self.audio_path = audio_path\n",
    "    \n",
    "    def process_audio(self, signal, new_sr):\n",
    "        try:\n",
    "            # right pad if necessary\n",
    "            length_signal = signal.shape[1]\n",
    "            if length_signal < 53363:\n",
    "                num_missing_samples = 53363 - length_signal\n",
    "                last_dim_padding = (0, num_missing_samples)\n",
    "                signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "            elif length_signal > 53363:\n",
    "                signal = signal[:, :53363]\n",
    "            return signal\n",
    "        except Exception as e:\n",
    "            # Manejar errores aquí, por ejemplo, imprimir un mensaje de error y devolver un tensor vacío\n",
    "            print(f\"Error al procesar el audio: {str(e)}\")\n",
    "            return torch.zeros((1, 53363))\n",
    "\n",
    "    def normalize_layer(self, feats):\n",
    "        with torch.no_grad():\n",
    "            feats = torch.nn.functional.layer_norm(feats, feats.shape)\n",
    "        return feats\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Intenta cargar el archivo de audio\n",
    "        feats, _ = torchaudio.load(self.audio_path + self.filenames[index])\n",
    "            \n",
    "        # Procesa el audio y normalízalo\n",
    "        feats = self.process_audio(feats, 16000)\n",
    "        feats = self.normalize_layer(feats)\n",
    "\n",
    "        # Asigna una etiqueta de destino basada en el nombre del archivo\n",
    "        if 'EXPL' in self.filenames[index]:\n",
    "            target = torch.tensor(int('00')).long() \n",
    "        elif 'HB' in self.filenames[index]:\n",
    "            target = torch.tensor(int('01')).long() \n",
    "        elif 'LP' in self.filenames[index]:\n",
    "            target = torch.tensor(int('02')).long()\n",
    "        elif 'TRBA' in self.filenames[index]:\n",
    "            target = torch.tensor(int('03')).long()\n",
    "        elif 'TRESP' in self.filenames[index]:\n",
    "            target = torch.tensor(int('04')).long()\n",
    "        elif 'VLP' in self.filenames[index]:\n",
    "            target = torch.tensor(int('05')).long()\n",
    "        elif 'VT' in self.filenames[index]:\n",
    "            target = torch.tensor(int('06')).long()\n",
    "\n",
    "        return {\"input_values\": feats, \"target\": target}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = os.listdir('D:/USFQ/Noveno Semestre/Proyecto Integrador/Project/QuakeWavNet/data/train/')\n",
    "X_test = os.listdir('D:/USFQ/Noveno Semestre/Proyecto Integrador/Project/QuakeWavNet/data/test/')\n",
    "X_val = os.listdir('D:/USFQ/Noveno Semestre/Proyecto Integrador/Project/QuakeWavNet/data/validation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002a32d9f7576599e5f41f7333c0098_BDF_BNAS_VLP_TYPE1.wav\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ANSYN_Dataset_SE(X_train, 'D:/USFQ/Noveno Semestre/Proyecto Integrador/Project/QuakeWavNet/data/train/')\n",
    "val_dataset =  ANSYN_Dataset_SE(X_val, 'D:/USFQ/Noveno Semestre/Proyecto Integrador/Project/QuakeWavNet/data/validation/')\n",
    "test_dataset = ANSYN_Dataset_SE(X_test, 'D:/USFQ/Noveno Semestre/Proyecto Integrador/Project/QuakeWavNet/data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequência de amostragem aceita pelo modelo: 16000\n",
      "Input values dimensão: torch.Size([1, 1, 53363])\n",
      "{'input_values': tensor([[[-0.7322, -0.7037, -0.6936,  ...,  0.3114,  0.3114,  0.3114]]]), 'attention_mask': tensor([[1]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"Frequência de amostragem aceita pelo modelo: {target_sampling_rate}\")\n",
    "# Conferindo se os dados de entrada não geram erro no processor\n",
    "inputs = processor(train_dataset[5][\"input_values\"], sampling_rate=target_sampling_rate, return_tensors=\"pt\")\n",
    "print(f'Input values dimensão: {inputs[\"input_values\"].shape}')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões de entrada do modelo:\n",
      "Dimensões de saída do modelo: \n",
      " torch.Size([1, 333, 768])\n"
     ]
    }
   ],
   "source": [
    "print('Dimensões de entrada do modelo:')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print('Dimensões de saída do modelo: \\n',last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"target\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        #print('batch', batch)\n",
    "        with self.processor.as_target_processor(): labels_batch = self.processor.pad( label_features, padding=True,max_length=self.max_length_labels,pad_to_multiple_of=self.pad_to_multiple_of_labels,return_tensors=\"pt\",)\n",
    "        print('labels_batch', labels_batch)\n",
    "\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "\n",
    "        batch[\"target\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(processor=processor,\n",
    "                                        #max_length=188,\n",
    "                                        padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de minibatches de treinamento: 3803\n",
      "Número de minibatches de validação: 816\n"
     ]
    }
   ],
   "source": [
    "batch_size = hparams[\"bs\"]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              collate_fn = data_collator,\n",
    "                              shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            collate_fn = data_collator,\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "print('Número de minibatches de treinamento:', len(train_dataloader))\n",
    "print('Número de minibatches de validação:', len(val_dataloader))\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "#print(batch)\n",
    "\n",
    "x_train, y_train = batch['input_values'], batch['target']\n",
    "print(\"\\nDimensões dos dados de um minibatch - Audio:\", x_train.size())\n",
    "# print(\"\\nDimensões dos dados de um minibatch:\", padding_mask.size())\n",
    "print(\"\\nDimensões dos dados de um minibatch - Target:\", y_train.size())\n",
    "print(\"Valores mínimo e máximo entrada: \", torch.min(x_train), torch.max(x_train))\n",
    "print(\"Valores mínimo e máximo saída: \", torch.min(y_train), torch.max(y_train))\n",
    "print(\"Tipo dos dados dos áudios:         \", type(x_train))\n",
    "print(\"Tipo das classes das classes:       \", type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 7\n",
    "f1 = F1Score(num_classes=n_classes, average='macro', task='multiclass')\n",
    "accuracy = Accuracy(num_classes=n_classes,task='multiclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2_sound_detection(pl.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hparams.update(hparams)\n",
    "\n",
    "        self.freeze_finetune_updates = hparams[\"freeze_finetune_updates\"]\n",
    "\n",
    "        #self.model = model4c\n",
    "        self.val_f1_scores = []\n",
    "        self.test_f1_scores = []\n",
    "        \n",
    "        self.model = Wav2Vec2_ChannelModel.from_pretrained(hparams[\"pretrained\"],\n",
    "                                                 conv_dim = (512, 512, 512, 512, 512, 512),\n",
    "                                                 conv_stride = (5, 2, 2, 2, 2, 2),\n",
    "                                                 conv_kernel = (10, 3, 3, 3, 3, 2),\n",
    "                                                 num_feat_extract_layers = 6,\n",
    "                                                 apply_spec_augment=hparams[\"apply_mask\"],\n",
    "                                                 #mask_time_length=hparams[\"mask_time_length\"],\n",
    "                                                 num_input_channels = 1,\n",
    "                                                 ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "        # self.model.feature_extractor._freeze_parameters()\n",
    "\n",
    "        # freeze base-model\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.projector = nn.Linear(self.model.config.hidden_size, self.model.config.classifier_proj_size)\n",
    "        n_classes = 7\n",
    "        self.final_layer = nn.Linear(self.model.config.classifier_proj_size, n_classes)\n",
    "\n",
    "    def forward(self, samples):\n",
    "\n",
    "        ft = self.freeze_finetune_updates <= self.trainer.global_step\n",
    "\n",
    "        with torch.no_grad() if not ft else contextlib.ExitStack():\n",
    "            hidden_states = self.model(**samples).last_hidden_state\n",
    "\n",
    "        padding_mask = self.model._get_feature_vector_attention_mask(hidden_states.shape[1], samples[\"attention_mask\"])\n",
    "\n",
    "        hidden_states[~padding_mask] = 0.0\n",
    "\n",
    "        pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n",
    "\n",
    "        proj_pooled = self.projector(pooled_output)\n",
    "\n",
    "        preds = self.final_layer(proj_pooled)\n",
    "\n",
    "        return F.log_softmax(preds, dim=1)\n",
    "\n",
    "    def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):\n",
    "        output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "        batch_size = attention_mask.shape[0]\n",
    "\n",
    "        attention_mask = torch.zeros(\n",
    "            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n",
    "        )\n",
    "\n",
    "        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n",
    "        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n",
    "        return attention_mask\n",
    "        \n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "\n",
    "        y_value = train_batch.pop(\"target\")\n",
    "        log_softs = self.forward(train_batch)\n",
    "    \n",
    "\n",
    "        loss = F.nll_loss(log_softs, y_value)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        self.log('loss_step', loss, on_step=True, prog_bar=True)\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        \n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "\n",
    "        y_value = val_batch.pop(\"target\")\n",
    "\n",
    "        log_softs = self.forward(val_batch)\n",
    "        preds = torch.argmax(log_softs, dim=1)\n",
    "\n",
    "        val_acc = accuracy(preds.cpu(), y_value.cpu())\n",
    "        val_f1 = f1(preds.cpu(), y_value.cpu())\n",
    "        val_loss = F.nll_loss(log_softs, y_value)\n",
    "\n",
    "        self.log('val_acc', val_acc, prog_bar=True)\n",
    "        self.log('val_f1', val_f1, prog_bar=True)\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "\n",
    "        return {\"val_acc_step\": val_acc, \"val_f1_step\": val_f1, \"val_loss_step\": val_loss}\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        acc_mean = torch.stack([x['val_acc_step'] for x in outputs]).mean()\n",
    "        f1_mean = torch.stack([x['val_f1_step'] for x in outputs]).mean()\n",
    "        loss_mean = torch.stack([x['val_loss_step'] for x in outputs]).mean()\n",
    "\n",
    "        self.log(\"val_acc\", acc_mean, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1_mean, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss_mean, prog_bar=True)\n",
    "\n",
    "        self.val_f1_scores.append(f1_mean)\n",
    "    import torch.functional as F\n",
    "    \n",
    "    \"\"\"def validation_epoch_end(self, outputs):\n",
    "        acc_mean = torch.stack([x['val_acc_step'] for x in outputs]).mean()\n",
    "        f1_mean = torch.stack([x['val_f1_step'] for x in outputs]).mean()\n",
    "        loss_mean = torch.stack([x['val_loss_step'] for x in outputs]).mean()\n",
    "\n",
    "        self.log(\"val_acc\", acc_mean, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1_mean, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss_mean, prog_bar=True)\"\"\"\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "\n",
    "        y_value = test_batch.pop(\"target\")\n",
    "\n",
    "        log_softs = self.forward(test_batch)\n",
    "        preds = torch.argmax(log_softs, dim=1)\n",
    "\n",
    "        test_acc = accuracy(preds.cpu(), y_value.cpu())\n",
    "        test_f1 = f1(preds.cpu(), y_value.cpu())\n",
    "        test_loss = F.nll_loss(log_softs, y_value)\n",
    "\n",
    "        self.log('test_acc', test_acc, prog_bar=True)\n",
    "        self.log('test_f1', test_f1, prog_bar=True)\n",
    "        self.log('test_loss', test_loss, prog_bar=True)\n",
    "\n",
    "        return {\"test_acc_step\": test_acc, \"test_f1_step\": test_f1,  \"test_loss_step\": test_loss}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        acc_mean = torch.stack([x['test_acc_step'] for x in outputs]).mean()\n",
    "        f1_mean = torch.stack([x['test_f1_step'] for x in outputs]).mean()\n",
    "        loss_mean = torch.stack([x['test_loss_step'] for x in outputs]).mean()\n",
    "\n",
    "        self.log(\"test_acc\", acc_mean, prog_bar=True)\n",
    "        self.log(\"test_f1\", f1_mean, prog_bar=True)\n",
    "        self.log(\"test_loss\", loss_mean, prog_bar=True)\n",
    "        \n",
    "        self.test_f1_scores.append(f1_mean)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(),\n",
    "                         lr=self.hparams[\"lr\"],\n",
    "                         betas=(0.9,0.98),\n",
    "                         eps=1e-6,\n",
    "                         weight_decay=self.hparams[\"w_decay\"])\n",
    "\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer,\n",
    "                                                  eta_min=0,\n",
    "                                                  warmup_start_lr=self.hparams[\"lr\"],\n",
    "                                                  warmup_epochs=self.hparams[\"warmup_epochs\"],\n",
    "                                                  max_epochs=self.hparams[\"max_epochs\"])\n",
    "\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjMWEyNTJlZS05ZDI5LTQzZjktYTkzNy00MDczMmZhODU3OWUifQ==\",\n",
    "    project='kgrosero/IA025-Project-wav2vec2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2_ChannelModel: ['lm_head.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2_ChannelModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2_ChannelModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2_ChannelModel were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/tmp/ipykernel_6728/949965054.py:166: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer,\n",
      "\n",
      "  | Name        | Type                  | Params\n",
      "------------------------------------------------------\n",
      "0 | model       | Wav2Vec2_ChannelModel | 93.8 M\n",
      "1 | projector   | Linear                | 196 K \n",
      "2 | final_layer | Linear                | 1.8 K \n",
      "------------------------------------------------------\n",
      "94.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "94.0 M    Total params\n",
      "376.184   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9acfbee8af04ed88e2ffa2913f8a5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d69c40cee64d45a0faf9145c031005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batchlabels_batch    {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "https://app.neptune.ai/kgrosero/IA025-Project-wav2vec2/e/IAP-54\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bcf1abb3864b57bc0d0bdfbecb8065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " \n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9281823d5b42f78a25a241aca5a32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " \n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "Unexpected error occurred in Neptune background thread: Killing Neptune asynchronous thread. All data is safe on disk and can be later synced manually using `neptune sync` command.\n",
      "labels_batchlabels_batch labels_batch "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NeptuneAsyncOpProcessor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 230, in run\n",
      "    super().run()\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/threading/daemon.py\", line 53, in run\n",
      "    self.work()\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 246, in work\n",
      "    self.process_batch([element.obj for element in batch], batch[-1].ver)\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/threading/daemon.py\", line 76, in wrapper\n",
      "    result = func(self_, *args, **kwargs)\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/operation_processors/async_operation_processor.py\", line 259, in process_batch\n",
      "    processed_count, errors = self._processor._backend.execute_operations(\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 467, in execute_operations\n",
      "    self._execute_upload_operations_with_400_retry(\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 571, in _execute_upload_operations_with_400_retry\n",
      "    return self._execute_upload_operations(\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 525, in _execute_upload_operations\n",
      "    upload_errors = upload_file_attribute(\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/backends/hosted_file_operations.py\", line 115, in upload_file_attribute\n",
      "    _multichunk_upload_with_retry(\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/backends/hosted_file_operations.py\", line 286, in _multichunk_upload_with_retry\n",
      "    return _multichunk_upload(upload_entry, swagger_client, query_params, multipart_config, urlset)\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/internal/backends/hosted_file_operations.py\", line 331, in _multichunk_upload\n",
      "    for idx, chunk in enumerate(chunker.generate()):\n",
      "  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/neptune/common/storage/datastream.py\", line 72, in generate\n",
      "    if last_change and last_change < os.stat(self._filename).st_mtime:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/cslab03/Desktop/QuakeWavNet/.neptune/None/version_None/checkpoints/epoch=0-step=3.ckpt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc3bbcf340748a0b886c3c1594fb863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b08aef022941a38c18507f86591978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch\n",
      "  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb23e84c939c4e7ba7b6e4e5162176db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}  \n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4def66d38e459f911e2805500ee412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " \n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0daacc0c23ec4f1d935114f8349a5c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d169177e577a4537aecbb9c5cbe5f24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b029d5669b274bc0a0f28c6383a263d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06df011241bb4c35b96128b6ce4be9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9cd10d2d4141419e829f28a5b9c13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0805aa4645674b90b376175f47a75642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}  \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f827b4b8214ed6ba3d57cce984ca59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a267e2f0d23494c8ae8e53bd81bb07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "\n",
      " {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batchlabels_batch    {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch\n",
      "  {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eb0b993e2645e3810d652d4ca55eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " \n",
      "\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61c234ea430407db00ac2237a588c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9945df4ef54a7581f232be82ba55eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "  labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a055afefaa9a402db943ae164f23178c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b3f624684f4c939dee8437b5db46d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b0e264988431aa69b2bb7387c2438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd677a1a3874bfc85c3cabfbd366ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch\n",
      "  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ac6e7c9a9541d886d14ca396c58067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cabdda152804c0cb0d0e59763b14fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7587965d32584e2292e5238675d38aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch\n",
      "  labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06c3b35421a4504b0214636bdd4f719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch\n",
      "  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      " \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feca81a701043c5b95a65ec8bbc8b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch\n",
      "  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65b649e095944ffa04d3a813e3c4357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      " {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c76eaadfdb54a3d828aed78474b163a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314d0cc1575d4a939b34583936737f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c8b2d916744ee9bd51c4c0b4628054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f5e3c5441d438fb8ee3ff827a6c42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      " labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a689a154840c48b9979b263466e26e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b50cd087e44c6785eda6e6490fbfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424471a7439f42f1914a30395f538669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fddaacbc00248a1bf45afacfcbc96e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efe23a2dd464eb68043a70d8558d23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch \n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9c4237a71b40f286c435d154a8e959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b390cf637a4b3b9cd3548936265ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839870b60252490594eb58dcec794fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba40a313d7a4441851d9fa9d7077b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e01ed345e04a33b6e60a9032e10109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch labels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028f9003bb4b4a74b1eca2ebdbe7dbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2497766593854ec790ffdd9f54913fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afdfaf9a7c4d7e8648f1639caad4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batchlabels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158629d7bf714214b84da2b09fc6bde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4052b02662c54f868b2d6e715a9addcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c094f4d3f546e8824b8f58e4331cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e224e4c31cf44d799887cd23b8787f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625898abb0b043c18f0460d1a1865d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6d738c893441a58c20e94f7bf0303f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d48223f82124cccb13539441f7e3570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch labels_batchlabels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00914c585a204c20b6c240e862aa7687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " \n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "labels_batch  {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7e190d77b54b0ebfdf8ba4e5a415fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd14aef1d3946caa5f802fa5ebb0f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " labels_batchlabels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73683190ee134057be4a081f2a0e4c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be0a36542cb46ecb528e275195c1d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a7680f69b746a1ba1f09794b20d320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "labels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e490c6389c540088a6544fe66410564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "\n",
      " {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e17563a86a4c129d82289de1fd44cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}  \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f8601633374e7996029aa442faf978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4afc77d838e4396b7b5129941bb8daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90df239391ff40b88d34fbe0cb4ee880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901010b1a034448aa147d5dde939fc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944d2fcb00f40c2b706e077f2f2e7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6a36bdbb024415bc4e5a73de2bfb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch\n",
      "\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da3fe3d6f3c41cb99d2a5eed78ece4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba48fc3d777413e927fbf14ab831fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c4b30f5e424c52898d662e5c16b35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62dec50a5b04de183653c7f2acef04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch\n",
      " {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7724825709e24d5e8186126bbd77e409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1015943f6a4841a51060cd8e9ce301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a0fd603cb748d3ad47bde9484a16e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6728f1f53c402fb295cfe409879c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b670cbd74347f1809d0222fbe9e2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " labels_batchlabels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01caaab81334b389d8b68354a0caa6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch labels_batch labels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f19e136f7546608d30f9c7d71f9511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch\n",
      "\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a303e9ec57747e18adefcb587063e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc1f020c3d14cc79da947cb7b58fd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1c78b711774fd79a3eea79ff68c9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " \n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98c94f53fd14549b533c1d5e2e611f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f2e4a5e71b47668c3cd05fa76b30d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c064b16c05c14dceb268e3c986e5b248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d79ec201a347ddafe5ea96d9010562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  \n",
      "\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad51e56ce4094f9e8a280b11513ee0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ddd9951ed846e9b9012e9d2303a196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5ae8ab59104e6d81508b670a48f765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      " labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7622b3ec117d4b89ba4719cc125f9918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch labels_batch{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04cdfe730c84e9e814a04f3219c9e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      " labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch  {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2d58ed7a20455f8ac1d4179d553fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e626c1efd5541338dacbcdacb74cd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      " {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "labels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc12b062424a078e5f5b50e243fbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae557f6a77e4bdf86588198db705300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a389694876d34d61bc9be0594ff05797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3447c1dbecff4826b474b68fa46199b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0417f51c87d04c23b61e08bd9c354b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batchlabels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b82f61ca7f4ce9945dc5220659248c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batchlabels_batch    {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} labels_batch\n",
      " {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6773b1d8434f1f95b91af37823ef1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batchlabels_batch   {'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batchlabels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941e5c13fe5d44cba0b90005db1d72c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch labels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      "{'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc67a0648a564efdb9aaf5164ddc8363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batchlabels_batch  {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch\n",
      " labels_batch {'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batchlabels_batch{'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}  \n",
      "{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 2, 6, 2, 6, 2, 6, 6, 6, 6, 2, 1, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 6, 2, 2, 1, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}labels_batch\n",
      " {'input_ids': tensor([2, 2, 1, 2, 2, 2, 6, 2, 1, 6, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 6, 1, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 2, 1, 2, 2, 1, 2, 6, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 4, 1, 2, 6, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 6, 6, 6, 6, 2, 6, 2, 6, 6, 2, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 4, 2, 2, 2, 6, 2, 2, 6, 2, 2, 0, 6, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194254190f0841aab029c7192860502d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([1, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batchlabels_batch  labels_batch  {'input_ids': tensor([0, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "{'input_ids': tensor([2, 2, 6, 2, 6, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 6, 6, 2, 6, 6, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 1, 2, 2, 6, 2, 6, 2, 6, 4, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 1, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2_sound_detection(hparams)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     logger=neptune_logger,\n",
    "                     max_epochs=100,\n",
    "                    overfit_batches=3,\n",
    "                    log_every_n_steps = 1)\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "del model, trainer # Para não ter estouro de mémoria da GPU\n",
    "#gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2_ChannelModel: ['lm_head.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2_ChannelModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2_ChannelModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2_ChannelModel were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in /home/cslab03/Desktop/QuakeWavNet/Results: []\n",
      "Saving checkpoints to /home/cslab03/Desktop/QuakeWavNet/Results\n"
     ]
    }
   ],
   "source": [
    "pl_model= Wav2Vec2_sound_detection(hparams=hparams)\n",
    "checkpoint_path = '/home/cslab03/Desktop/QuakeWavNet/Results/'\n",
    "checkpoint_dir = os.path.dirname(os.path.abspath(checkpoint_path))\n",
    "print(f'Files in {checkpoint_path}: {os.listdir(checkpoint_path)}')\n",
    "print(f'Saving checkpoints to {checkpoint_path}')\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(filename=hparams[\"version\"],\n",
    "                                                  dirpath=checkpoint_dir,\n",
    "                                                  save_top_k=1,\n",
    "                                                  verbose = True,\n",
    "                                                  monitor=\"val_f1\", mode=\"max\")\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_f1\", patience=hparams[\"patience\"], mode='max')\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1,\n",
    "                     precision=16,\n",
    "                     logger=neptune_logger,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     accumulate_grad_batches=hparams[\"accum_grads\"],\n",
    "                     enable_checkpointing=True,\n",
    "                     callbacks=[early_stop_callback, lr_monitor, checkpoint_callback],\n",
    "                     max_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/tmp/ipykernel_6728/949965054.py:166: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer,\n",
      "\n",
      "  | Name        | Type                  | Params\n",
      "------------------------------------------------------\n",
      "0 | model       | Wav2Vec2_ChannelModel | 93.8 M\n",
      "1 | projector   | Linear                | 196 K \n",
      "2 | final_layer | Linear                | 1.8 K \n",
      "------------------------------------------------------\n",
      "94.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "94.0 M    Total params\n",
      "188.092   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e411153fc6d4990b1543c3166425294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_batch {'input_ids': tensor([6, 2, 4, 2, 6, 6, 6, 6, 6, 2, 6, 2, 6, 6, 1, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batchlabels_batch  {'input_ids': tensor([2, 2, 6, 2, 2, 2, 1, 2, 6, 2, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}{'input_ids': tensor([2, 6, 2, 6, 6, 6, 2, 1, 6, 2, 2, 6, 2, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "\n",
      "labels_batch labels_batch {'input_ids': tensor([2, 6, 2, 2, 6, 6, 6, 6, 6, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'input_ids': tensor([2, 2, 6, 1, 2, 6, 2, 1, 6, 6, 6, 6, 2, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch labels_batch{'input_ids': tensor([6, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])} \n",
      "{'input_ids': tensor([6, 2, 1, 2, 6, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([4, 2, 6, 2, 2, 6, 2, 2, 6, 2, 6, 6, 1, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 4, 2, 6, 2, 2, 6, 2, 6, 2, 6, 2, 2, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 6, 2, 6, 2, 2, 6, 6, 2, 2, 2, 6, 2, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 2, 2, 6, 6, 6, 2, 2, 6, 6, 6, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 2, 2, 6, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([1, 6, 4, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 2, 6, 2, 2, 6, 6, 6, 2, 6, 6, 2, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 2, 4, 2, 6, 2, 6, 6, 6, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 6, 2, 2, 2, 6, 6, 2, 6, 6, 2, 1, 2, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 2, 6, 2, 2, 6, 2, 2, 6, 6, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 1, 2, 6, 6, 2, 6, 6, 2, 6, 2, 2, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 4, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([6, 2, 6, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 6, 2, 6, 6, 2, 6, 6, 2, 2, 2, 2, 1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 6, 6, 6, 6, 6, 2, 6, 2, 6, 2, 6, 2, 6, 2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "labels_batch {'input_ids': tensor([2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 0, 2, 2, 6]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_6728/419823214.py\", line 28, in __getitem__\n    feats, _ = torchaudio.load(self.audio_path + self.filenames[index])\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 256, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 30, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from /home/cslab03/Desktop/QuakeWavNet/output/train/ed31672125758dac5313e525af133825_BHN_BTAM_LP.wav\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cslab03/Desktop/QuakeWavNet/QuakeWavNet.ipynb Cell 33\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cslab03/Desktop/QuakeWavNet/QuakeWavNet.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m val_f1_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cslab03/Desktop/QuakeWavNet/QuakeWavNet.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cslab03/Desktop/QuakeWavNet/QuakeWavNet.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(pl_model, train_dataloader, val_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:187\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    186\u001b[0m     batch_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(data_fetcher)\n\u001b[1;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     batch_idx, batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetching_function()\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[39m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fetch_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataloader_iter)\n\u001b[1;32m    266\u001b[0m         \u001b[39m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatches\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[1;32m    281\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:571\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    566\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[1;32m    568\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_next_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_iters)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/pytorch_lightning/trainer/supporters.py:583\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    575\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_to_collection(loader_iters, Iterator, \u001b[39mnext\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/lightning_utilities/core/apply_func.py:51\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     53\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_6728/419823214.py\", line 28, in __getitem__\n    feats, _ = torchaudio.load(self.audio_path + self.filenames[index])\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 256, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/home/cslab03/anaconda3/envs/coEnv/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 30, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from /home/cslab03/Desktop/QuakeWavNet/output/train/ed31672125758dac5313e525af133825_BHN_BTAM_LP.wav\n"
     ]
    }
   ],
   "source": [
    "# Definir listas para almacenar las métricas por época\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer.fit(pl_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(checkpoint_path + hparams[\"version\"]):\n",
    "    print('Saving processor to: ' + checkpoint_path + hparams[\"version\"])\n",
    "    processor.save_pretrained(checkpoint_path + hparams[\"version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback.best_model_path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Después de completar el entrenamiento\n",
    "epochs = list(range(1, len(pl_model.val_f1_scores) + 1))\n",
    "plt.plot(epochs, pl_model.val_f1_scores, marker='o')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score por Época')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = checkpoint_callback.best_model_path\n",
    "# best_model = \"/content/drive/MyDrive/Wav2Vec2_ORVP/wav2vec2_huggingface_fairseq_orvp_test1-epoch=4-step=23459.ckpt\"\n",
    "print(best_model)\n",
    "test_model = Wav2Vec2_sound_detection.load_from_checkpoint(best_model, hparams=hparams).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            collate_fn = data_collator,\n",
    "                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
